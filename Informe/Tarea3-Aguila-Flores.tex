\documentclass[letter, 10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[dvips]{graphicx}
\usepackage{graphicx}
\usepackage{subfigure} % subfiguras
\DeclareGraphicsExtensions{.bmp,.png,.pdf,.jpg}
\usepackage{xcolor,listings}%color support for listings
\usepackage{epstopdf}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{url}
\usepackage{caption}
\usepackage[justification=centering]{caption}
\usepackage{cite}
\usepackage[top=3cm,bottom=3cm,left=3.5cm,right=3.5cm,footskip=1.5cm,headheight=1.5cm,headsep=.5cm,textheight=3cm]{geometry}



\begin{document}



\title{Análisis Inteligente de Datos \\ \begin{Large}Tarea 3\end{Large}}
\author{Paulina Aguila - Felipe Flores}
\date{14 de julio de 2016}

\maketitle


\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.2\textwidth]{Images/Isotipo-Negro.png}\\
\end{center}
\end{figure}
\vspace{2cm}

\section{Reducción de Dimensionalidad para Clasificación}

La reducción de dimensionalidad es un proceso que tiene mucha importancia dentro del análisis de datos, ya que ayuda a la visualización y la exploración de los datos, también reduce el costo computacional del procesamiento de los datos debido a que son menos dimensiones. Otro beneficio importante de la reducción de dimensionalidad, es que reduce significativamente el riesgo de \textit{overfitting} o sobre ajuste del modelo.\\

En esta primera sección, se trabajará con datos sobre sonidos fonéticos que debe ser identificados con vocales del inglés británico. Los datos se representan en un espacio de 10 características ($d=10$), en donde 528 registros corresponden a datos de entrenamiento y 462 son datos de prueba. Los autores reportan que el mejor desempeño corresponde a un 56\% de accuracy y se alcanza con un modelo de vecinos más cercanos y una red neuronal artificial de radio basal.\\

A través del lenguaje de programación Python, se cargan los datos de la fuente \cite{D1} y se llevan a un dataframe de entrenamiento con 528 registros y un dataframe de prueba con 462 registros.\\

Para cada conjunto de datos (entrenamiento y test), se deben normalizar los datos. Este es un paso muy importante, ya que permite ajustar la escala de las variables a la varianza de la unidad, lo que hace que los valores de datos que se encuentran ubicados en los extremos, no ejerzan un peso excesivo en la función objetivo.\\

Utilizando PCA (Análisis de Componentes Principales) se genera una representación en dos dimensiones para el dataset inicial (10 dimensiones). La Figura \ref{G1.1}, muestra la clasificación que realiza PCA.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{Images/figure_1.png}
\caption{Gráfica que muestra las dos componentes principales de PCA diferenciando con distintos colores las 9 clases.}
\label{G1.1}
\end{center}
\end{figure}

Utilizando LDA (Linear Discriminant Analysis) se genera una representación en dos dimensiones para el dataset inicial (10 dimensiones). La Figura \ref{G1.2}, muestra la clasificación que realiza LDA.\\

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{Images/figure_2.png}
\caption{Gráfica que muestra las dos componentes principales de LDA diferenciando con distintos colores las 9 clases.}
\label{G1.2}
\end{center}
\end{figure}

Al analizar las Figuras \ref{G1.1} y \ref{G1.2}, se puede observar que en ambas se redujo la dimensionalidad de 10 a 2 componentes. Sin embargo, con PCA se observa que las clases no están separadas lo suficiente como para diferenciarlas, pero al utilizar LDA, se puede ver que las clases se separan más entre sí. Esto se puede deber a que LDA es un método de clasificación que en este caso se utiliza para reducir dimensionalidad y evitar el \textit{overfitting}.\\

Para el caso de querer clasificar un registro $x$ escogido aleatóriamente considerando solo la probabilidad a priori de cada clase, se debe calcular la probabilidad de ocurrencia de cada clase, luego en base a la ecuación (1), se selecciona la clase que tiene mayor probabilidad.

\begin{equation}
j=\underset{i}{Argmax} (P(y=C_i))
\end{equation}

En el siguiente ítem, se utilizaron los métodos de clasificación LDA, QDA y KNN, sin realizar reducción de dimensionalidad. A continuación, la Tabla \ref{table:score} muestra un resumen con los valores del accuracy para cada uno de estos modelos tanto con los datos de entrenamiento como con los de test.

\begin{table}[!hbt] 
\begin{center}
\begin{tabular}{| l | c | c | c |} 
\hline
\textbf{} & \textbf{LDA} & \textbf{QDA} &\textbf{KNN (k=10)} \\ 
\hline 
Train set & 0.6837 & \textbf{0.9886} & 0.9318\\ 
Test set &0.4524 & 0.4156 & \textbf{0.4913}\\
\hline 
\end{tabular}
\caption{Recuadro con el score o accuracy para cada método aplicado a datos de entrenamiento o de prueba.} 
\label{table:score}
\end{center} 
\end{table}

De la Tabla \ref{table:score}, se puede ver que para el conjunto de datos de train el método que mejor se comporta es QDA, mientras que al aplicarlo a los datos de test el método KNN tiene la mejor accuracy.\\

La Figura \ref{G1.3} que se muestra a continuación, muestra un gráfico en donde se aplica el método KNN variando el valor de k desde 1 a 10, con respecto al score o accuracy. Se puede apreciar que cuando k=7 entonces el método KNN funciona mejor y tiene mejor score.\\

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/figure_3.png}
\caption{Gráfica que muestra las dos componentes principales de LDA diferenciando con distintos colores las 9 clases.}
\label{G1.3}
\end{center}
\end{figure}

Para la última parte, se realizó reducción de domiensionalidad con los métodos PCA y LDA, para cada uno de los cuales se fue variando la cantidad de dimensiones desde 1 hasta la máxima 10, además, para cada iteración se clasificó el modelo con LDA, QDA y KNN (k=7), calculando los errores de clasificación para los datos de entrenamiento y para los de test. Estos errores se graficaron obteniéndose los gráficos de las Figuras \ref{pca} y \ref{lda}.\\

\begin{figure}[h]
\begin{center}
\subfigure[LDA]{\includegraphics[width=0.6\textwidth]{Images/figure_4.png}}
\subfigure[QDA]{\includegraphics[width=0.6\textwidth]{Images/figure_5.png}}
\subfigure[KNN (k=7)]{\includegraphics[width=0.6\textwidth]{Images/figure_6.png}}
\caption{Gráficas de los errores de clasificación haciendo reducción de dimensionalidad con PCA con d dimensiones.} 
\label{pca}
\end{center}
\end{figure}

Para la Figura \ref{pca}, se puede ver que la cantidad de dimensiones igual a 7 es óptima ya que reduce el error de clasificación para los datos de test, comportándose de mejor manera el método LDA. Para la Figura \ref{lda}, se observa que con una cantidad de dimensiones igual a 2 se tiene un menor error para los datos de prueba, siendo el más estable LDA no variando mucho su error al aumentar de dimensionalidad.\\

\begin{figure}[h]
\begin{center}
\subfigure[LDA]{\includegraphics[width=0.6\textwidth]{Images/figure_7.png}}
\subfigure[QDA]{\includegraphics[width=0.6\textwidth]{Images/figure_8.png}}
\subfigure[KNN (k=7)]{\includegraphics[width=0.6\textwidth]{Images/figure_9.png}}
\caption{Gráficas de los errores de clasificación haciendo reducción de dimensionalidad con LDA con d dimensiones.} 
\label{lda}
\end{center}
\end{figure}

\section{Análisis de Opiniones sobre Películas}

PIPE


\end{document} 